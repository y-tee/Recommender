{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implicit ALS\n",
    "\n",
    "We usually consider using ALS on a set of user/product ratings. But what if the data isn't so self explanatory?\n",
    "\n",
    "### A day trip to the library\n",
    "Consider, for example, the data collected by a local library. The library records which users took out each books and how long they kept the books before returning them. \n",
    "\n",
    "As such, we have no explicit indication that a user liked or disliked the books they took out - Just because you borrowed a book does not mean that you enjoyed it, or even read it.\n",
    "Furthermore, the missing data is of interest - the fact that a user has not taken out a specific book could indicate that they dislike that genre, or that they haven't been to that section of the library.\n",
    "\n",
    "Furthermore the same user action could have many different causes. Suppose you withdraw a book three times. That might indicate that you loved the book, but it may also indicate that the book doesn't appeal to you as strongly as some other books you withdrew so you never got round to reading it the first two times.\n",
    "\n",
    "To make the situation even worse, implicit data is often dirty. For example, a user may withdraw a library book for their child using their account, or they may accidentally pick up a book that was sitting on the counter. \n",
    "\n",
    "### The solution\n",
    "Based on the standard ALS implementation, [Hu et al. (2008)](https://www.google.ca/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwi899eAu6baAhUurlkKHaVvB6UQFggsMAA&url=http%3A%2F%2Fyifanhu.net%2FPUB%2Fcf.pdf&usg=AOvVaw3WIcPGTpxR8m7C32F8whFx) presented a methodolgy for carrying out ALS when dealing with implicit data. \n",
    "\n",
    "The general idea is that we have some recorded observations $r_{u,i}$ denoting the level of interaction user $u$ had with product $i$. For example, if a user $1$ borrowed book $4$ once we may set $r_{1,4}=1$. Alternatively we may wish to allow $r_{u,i}$ to hold information about how many days the book was borrowed for. (There is a lot of freedom in this set up, so we need to make some data specific decisions regarding how we will select $r_{u,i}$).\n",
    "\n",
    "Given the set of observations $r_{u,i}$, a binary indicator $p_{u,i}$ is introduced where:\n",
    "\n",
    "$ p_{u,i} = \\begin{cases} 1 & \\mbox{if } r_{i,j}>0 \\\\\n",
    "0 & \\mbox{otherwise.} \\end{cases} $\n",
    "\n",
    "\n",
    "A confidence parameter $\\alpha$ lets the user determine how much importance they wish to place on the recorded $r_{u,i}$. This leads to the introduction of $c_{u,i}$ which we take to be the confidence we have in the strength of user $u$'s reaction to product $i$: \n",
    "$c_{u,i} = 1 + \\alpha r_{u,i}$.\n",
    "\n",
    "Let $N_u$ denote the number of users, and $N_p$ denote the number of products. Let $k\\in \\mathbb{R}^+$ be a user defined number of factors. \n",
    "Now, in implicit ALS the goal is to find matrices $X\\in \\mathbb{R}^{N_u \\times k}$ and $Y\\in \\mathbb{R}^{N_p \\times k}$ such that the following cost function is minimised:\n",
    "\n",
    "$\\sum_{u,i} c_{u,i}(p_{u,i}-X_u^T Y_i)^2 + \\lambda (\\sum_u \\| X_u\\|^2 + \\sum_{i} \\| y_u\\|^2), $\n",
    "\n",
    "\n",
    "where\n",
    "$X_u$ is the $u$th row of X, \n",
    "$Y_i$ is the $i$th row of Y,\n",
    "\\lambda is a user defined parameter which prevents overfitting. \n",
    "\n",
    "With this minimisation at hand, we are able to recover estimates of $c_{u,i}$, and thus of $r_{u,i}$ for interactions which have not yet occured. \n",
    "\n",
    "### Let's get going\n",
    "We are going to run implicit ALS using the implementation given in the pyspark.mllib.recommendation module. \n",
    "\n",
    "The data we will be using can be found at http://www2.informatik.uni-freiburg.de/~cziegler/BX/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = '/opt/conda/bin/python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=implicitALS, master=local[*]) created by __init__ at <ipython-input-2-9e8547090bfd>:7 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-9e8547090bfd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"implicitALS\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.2.3-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \"\"\"\n\u001b[0;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.2.3-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    303\u001b[0m                         \u001b[1;34m\" created by %s at %s:%s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[1;32m--> 305\u001b[1;33m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[0;32m    306\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=implicitALS, master=local[*]) created by __init__ at <ipython-input-2-9e8547090bfd>:7 "
     ]
    }
   ],
   "source": [
    "#Set up a spark context\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext,  SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"implicitALS\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"new training list.csv\",'r') as f:\n",
    "    with open(\"updated training list.csv\",'w') as f1:\n",
    "        next(f) # skip header line\n",
    "        for line in f:\n",
    "            f1.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load in the data\n",
    "#The data is csv, with ';' as a delimiter, hence the split command. \n",
    "#The data has quote marks around all info, so I remove these with a replace mapping. \n",
    "#The first bit of data is user id, the second is the book isbn number, \n",
    "# and the third is the observation. \n",
    "ratings = sc.textFile('updated training list.csv').map(lambda x:x.split(\",\")) \\\n",
    "            .map(lambda x:(int(x[0]), int(x[3]), int(x[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingsdf=pd.read_csv('new training list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[49] at RDD at PythonRDD.scala:52"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the first 10 entries in the ratings file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 508, 1),\n",
       " (9, 34, 1),\n",
       " (12, 508, 1),\n",
       " (24, 522, 39),\n",
       " (29, 522, 1),\n",
       " (42, 1, 1),\n",
       " (42, 34, 1),\n",
       " (42, 628, 1),\n",
       " (42, 522, 2),\n",
       " (61, 508, 1)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS\n",
    "model=ALS.trainImplicit(ratings, rank=5, iterations=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at user 8. We wish to make predictions on what books the user will like, based on their interactions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rec(user_id,k):\n",
    "    predicted=model.recommendProducts(user_id,ratingsdf['New_id'].nunique())\n",
    "\n",
    "    stlist=[]\n",
    "    stexist=[]\n",
    "\n",
    "    for x in predicted:\n",
    "        st=(x[1],x[2])\n",
    "        stlist.append(st)\n",
    "    \n",
    "    df=ratingsdf[ratingsdf['user_id']==user_id]\n",
    "\n",
    "    for index,row in df.iterrows():\n",
    "        stold=row['New_id']\n",
    "        stexist.append(stold)\n",
    "    \n",
    "    finallist=[x for x in stlist if x[0] not in stexist]\n",
    "    final=finallist[0:k]\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, 0.174345308161444),\n",
       " (106, 0.12648781554226715),\n",
       " (3, 0.11752252422730258),\n",
       " (15, 0.06500946377697087),\n",
       " (153, 0.05178953708412176),\n",
       " (28, 0.04814923991183831),\n",
       " (5, 0.046985803697913664),\n",
       " (441, 0.04260351458528505),\n",
       " (52, 0.040151053789649185),\n",
       " (10, 0.03865064728491016)]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec(42,10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
