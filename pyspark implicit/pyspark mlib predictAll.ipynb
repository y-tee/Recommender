{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark import sql, SparkContext,  SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need to run sometimes for local or docker use\n",
    "# conf = SparkConf().setAppName(\"implicitALS\")\n",
    "# sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlC = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for local or docker use\n",
    "# df = sqlC.read.csv('new training list.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to replace this with s3 link\n",
    "df = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/new_training_list-004ea.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings=df.select(\"user_id\",\"New_id\",\"srcount\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate unique list of user_id+new_id for predictAll\n",
    "user=df.select(\"user_id\").distinct()\n",
    "st=df.select(\"New_id\").distinct()\n",
    "user_st=user.crossJoin(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not convert to rdd first \n",
    "# user_st=user_st.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only used in local or docker\n",
    "# df1 = sqlC.read.csv('user-item list.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_renamed = user_st.withColumnRenamed('user_id', 'user_id1').withColumnRenamed('New_id', 'New_id1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register temp table\n",
    "df.registerTempTable(\"df\")\n",
    "df1_renamed.registerTempTable('df1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all the right key, do not want all the st that is requested before\n",
    "removed=sqlC.sql(\"Select a.* from df1 a left join df b on (a.user_id1=b.user_id and a.New_id1=b.New_id) where (b.New_id is null and b.user_id is null)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed=removed.limit(removed.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed=removed.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS\n",
    "# rank is no features\n",
    "model = ALS.trainImplicit(ratings, rank=20, iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# huge elastic search table with all the service_type and all the confidence score\n",
    "final_table=model.predictAll(removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to push final_tabledf to somewhere\n",
    "final_tabledf=final_table.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tabledf.registerTempTable(\"final_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing and querying final table\n",
    "test=spark.sql(\"select * from test_final_table order by user desc, rating desc limit 500\")\n",
    "display(test.select('*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tried huge table with all the user_id but only 6 recommended items\n",
    "# taking too long\n",
    "# predicted = []\n",
    "\n",
    "# for i in user_list:\n",
    "#     predict=model.recommendProducts(i,6)\n",
    "#     predicted.append(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "name": "without_old then validate",
  "notebookId": 2178477997798079
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
