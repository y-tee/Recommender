{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implicit ALS\n",
    "\n",
    "We usually consider using ALS on a set of user/product ratings. But what if the data isn't so self explanatory?\n",
    "\n",
    "### The solution\n",
    "Based on the standard ALS implementation, [Hu et al. (2008)](https://www.google.ca/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwi899eAu6baAhUurlkKHaVvB6UQFggsMAA&url=http%3A%2F%2Fyifanhu.net%2FPUB%2Fcf.pdf&usg=AOvVaw3WIcPGTpxR8m7C32F8whFx) presented a methodolgy for carrying out ALS when dealing with implicit data. \n",
    "\n",
    "The general idea is that we have some recorded observations $r_{u,i}$ denoting the level of interaction user $u$ had with product $i$. For example, if a user $1$ borrowed book $4$ once we may set $r_{1,4}=1$. Alternatively we may wish to allow $r_{u,i}$ to hold information about how many days the book was borrowed for. (There is a lot of freedom in this set up, so we need to make some data specific decisions regarding how we will select $r_{u,i}$).\n",
    "\n",
    "Given the set of observations $r_{u,i}$, a binary indicator $p_{u,i}$ is introduced where:\n",
    "\n",
    "$ p_{u,i} = \\begin{cases} 1 & \\mbox{if } r_{i,j}>0 \\\\\n",
    "0 & \\mbox{otherwise.} \\end{cases} $\n",
    "\n",
    "\n",
    "A confidence parameter $\\alpha$ lets the user determine how much importance they wish to place on the recorded $r_{u,i}$. This leads to the introduction of $c_{u,i}$ which we take to be the confidence we have in the strength of user $u$'s reaction to product $i$: \n",
    "$c_{u,i} = 1 + \\alpha r_{u,i}$.\n",
    "\n",
    "Let $N_u$ denote the number of users, and $N_p$ denote the number of products. Let $k\\in \\mathbb{R}^+$ be a user defined number of factors. \n",
    "Now, in implicit ALS the goal is to find matrices $X\\in \\mathbb{R}^{N_u \\times k}$ and $Y\\in \\mathbb{R}^{N_p \\times k}$ such that the following cost function is minimised:\n",
    "\n",
    "$\\sum_{u,i} c_{u,i}(p_{u,i}-X_u^T Y_i)^2 + \\lambda (\\sum_u \\| X_u\\|^2 + \\sum_{i} \\| y_u\\|^2), $\n",
    "\n",
    "\n",
    "where\n",
    "$X_u$ is the $u$th row of X, \n",
    "$Y_i$ is the $i$th row of Y,\n",
    "\\lambda is a user defined parameter which prevents overfitting. \n",
    "\n",
    "With this minimisation at hand, we are able to recover estimates of $c_{u,i}$, and thus of $r_{u,i}$ for interactions which have not yet occured. \n",
    "\n",
    "### Let's get going\n",
    "We are going to run implicit ALS using the implementation given in the pyspark.mllib.recommendation module. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set up a spark context\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext,  SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"implicitALS\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"new training list.csv\",'r') as f:\n",
    "    with open(\"updated training list.csv\",'w') as f1:\n",
    "        next(f) # skip header line\n",
    "        for line in f:\n",
    "            f1.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load in the data\n",
    "#The data is csv, with ';' as a delimiter, hence the split command. \n",
    "#The data has quote marks around all info, so I remove these with a replace mapping. \n",
    "#The first bit of data is user id, the second is the book isbn number, \n",
    "# and the third is the observation. \n",
    "ratings = sc.textFile('updated training list.csv').map(lambda x:x.split(\",\")) \\\n",
    "            .map(lambda x:(int(x[0]), int(x[3]), int(x[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingsdf=pd.read_csv('new training list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[2] at RDD at PythonRDD.scala:52"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the first 10 entries in the ratings file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 508, 1),\n",
       " (9, 34, 1),\n",
       " (12, 508, 1),\n",
       " (24, 522, 39),\n",
       " (29, 522, 1),\n",
       " (42, 1, 1),\n",
       " (42, 34, 1),\n",
       " (42, 628, 1),\n",
       " (42, 522, 2),\n",
       " (61, 508, 1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS\n",
    "model=ALS.trainImplicit(ratings, rank=5, iterations=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at user 8. We wish to make predictions on what books the user will like, based on their interactions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rec(user_id,k):\n",
    "    predicted=model.recommendProducts(user_id,ratingsdf['New_id'].nunique())\n",
    "\n",
    "    stlist=[]\n",
    "    stexist=[]\n",
    "\n",
    "    for x in predicted:\n",
    "        st=(x[1],x[2])\n",
    "        stlist.append(st)\n",
    "    \n",
    "    df=ratingsdf[ratingsdf['user_id']==user_id]\n",
    "\n",
    "    for index,row in df.iterrows():\n",
    "        stold=row['New_id']\n",
    "        stexist.append(stold)\n",
    "    \n",
    "    finallist=[x for x in stlist if x[0] not in stexist]\n",
    "    final=finallist[0:k]\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, 0.16010313544290833),\n",
       " (106, 0.11411798313542837),\n",
       " (3, 0.11274498697259272),\n",
       " (15, 0.06136344494823071),\n",
       " (153, 0.045765681287896584),\n",
       " (28, 0.04278850291303096),\n",
       " (5, 0.04248322675899589),\n",
       " (441, 0.036879006074328625),\n",
       " (10, 0.03671808274779216),\n",
       " (11, 0.03584669533520355)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec(42,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
